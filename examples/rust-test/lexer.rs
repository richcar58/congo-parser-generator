//! Lexer implementation. Generated by CongoCC Parser Generator. Do not edit.

use crate::error::{ParseError, ParseResult};
use crate::tokens::{Token, TokenType, LexicalState, TokenSource};

/// The lexer/tokenizer for ArithmeticParser
pub struct Lexer {
    /// The input string being tokenized
    input: String,
    /// Current position in the input
    position: usize,
    /// Current lexical state
    state: LexicalState,
    /// All tokens generated so far
    tokens: Vec<Token>,
    /// Current line number (1-indexed)
    current_line: usize,
    /// Current column number (1-indexed)
    current_column: usize,
    /// Line start offsets for quick line/column lookup
    line_starts: Vec<usize>,
}

impl Lexer {
    /// Create a new lexer for the given input
    pub fn new(input: String) -> Self {
        let mut line_starts = vec![0];
        for (i, ch) in input.char_indices() {
            if ch == '\n' {
                line_starts.push(i + 1);
            }
        }

        Lexer {
            input,
            position: 0,
            state: LexicalState::DEFAULT,
            tokens: Vec::new(),
            current_line: 1,
            current_column: 1,
            line_starts,
        }
    }

    /// Get the next token from the input
    pub fn next_token(&mut self) -> ParseResult<Token> {
        // Skip whitespace and comments based on lexical state
        self.skip_ignored()?;

        if self.position >= self.input.len() {
            // Return EOF token
            return Ok(Token::new(
                TokenType::EOF,
                String::new(),
                self.position,
                self.position,
            ));
        }

        let start_pos = self.position;
        let start_line = self.current_line;
        let start_column = self.current_column;

        // Try to match each token type in order
        if let Some(token) = self.try_match_token(start_pos)? {
            return Ok(token);
        }

        // If no token matched, it's an error
        Err(ParseError::at_location(
            format!("Unexpected character: '{}'", self.current_char()),
            start_line,
            start_column,
        ))
    }

    /// Try to match a token at the current position
    fn try_match_token(&mut self, start_pos: usize) -> ParseResult<Option<Token>> {
        // Match literal
        if self.matches_string("+") {
            let image = "+".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::PLUS,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match literal
        if self.matches_string("-") {
            let image = "-".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::MINUS,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match literal
        if self.matches_string("*") {
            let image = "*".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::STAR,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match literal
        if self.matches_string("/") {
            let image = "/".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::SLASH,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match literal
        if self.matches_string("(") {
            let image = "(".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::LPAREN,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match literal
        if self.matches_string(")") {
            let image = ")".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::RPAREN,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match literal
        if self.matches_string(" ") {
            let image = " ".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::Token7,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match literal
        if self.matches_string("\t") {
            let image = "\t".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::Token8,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match literal
        if self.matches_string("\n") {
            let image = "\n".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::Token9,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match literal
        if self.matches_string("\r") {
            let image = "\r".to_string();
            let end_pos = self.position + image.len();
            for _ in 0..image.len() {
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::Token10,
                image,
                start_pos,
                end_pos,
            )));
        }

        // Match INTEGER token (digits)
        if self.current_char().is_ascii_digit() {
            let mut image = String::new();
            while self.position < self.input.len() && self.current_char().is_ascii_digit() {
                image.push(self.current_char());
                self.advance();
            }
            return Ok(Some(Token::new(
                TokenType::INTEGER,
                image,
                start_pos,
                self.position,
            )));
        }


        // No token matched
        Ok(None)
    }

    /// Skip whitespace and ignored tokens
    fn skip_ignored(&mut self) -> ParseResult<()> {
        while self.position < self.input.len() {
            let ch = self.current_char();

            // Skip whitespace
            if ch.is_whitespace() {
                self.advance();
                continue;
            }

            // TODO: Skip comments based on grammar definitions

            break;
        }
        Ok(())
    }

    /// Get the current character without consuming it
    fn current_char(&self) -> char {
        self.input[self.position..].chars().next().unwrap_or('\0')
    }

    /// Advance to the next character
    fn advance(&mut self) {
        if self.position < self.input.len() {
            let ch = self.current_char();
            self.position += ch.len_utf8();

            if ch == '\n' {
                self.current_line += 1;
                self.current_column = 1;
            } else {
                self.current_column += 1;
            }
        }
    }

    /// Peek ahead n characters without consuming
    fn peek(&self, n: usize) -> Option<char> {
        self.input[self.position..].chars().nth(n)
    }

    /// Check if current position matches a string
    fn matches_string(&self, s: &str) -> bool {
        self.input[self.position..].starts_with(s)
    }
}

impl TokenSource for Lexer {
    fn get_line_from_offset(&self, offset: usize) -> usize {
        // Binary search for the line containing this offset
        match self.line_starts.binary_search(&offset) {
            Ok(line) => line + 1,
            Err(line) => line,
        }
    }

    fn get_column_from_offset(&self, offset: usize) -> usize {
        let line_num = self.get_line_from_offset(offset);
        if line_num == 0 || line_num > self.line_starts.len() {
            return 1;
        }

        let line_start = self.line_starts[line_num - 1];
        offset.saturating_sub(line_start) + 1
    }
}
