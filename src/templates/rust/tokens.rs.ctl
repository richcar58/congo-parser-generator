//! Token types and definitions. Generated by ${generated_by}. Do not edit.

use std::fmt;

/// Token type enumeration
///
/// Represents all possible token types recognized by the lexer.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum TokenType {
[#list lexerData.regularExpressions as regexp]
    [#-- Clean up token names: remove leading underscore, convert _TOKEN_N to TokenN --]
    [#if regexp.label?starts_with("_TOKEN_")]
    /// Token: ${regexp.label}
        ${regexp.label?replace("_TOKEN_", "Token")},
    [#else]
    /// Token: ${regexp.label}
        ${regexp.label},
    [/#if]
[/#list]
[#list settings.extraTokenNames as tokenName]
    /// Token: ${tokenName}
    ${tokenName},
[/#list]
    /// Invalid token marker
    INVALID,
}

impl fmt::Display for TokenType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
[#list lexerData.regularExpressions as regexp]
            [#if regexp.label?starts_with("_TOKEN_")]
            TokenType::${regexp.label?replace("_TOKEN_", "Token")} => write!(f, "${regexp.label}"),
            [#else]
            TokenType::${regexp.label} => write!(f, "${regexp.label}"),
            [/#if]
[/#list]
[#list settings.extraTokenNames as tokenName]
            TokenType::${tokenName} => write!(f, "${tokenName}"),
[/#list]
            TokenType::INVALID => write!(f, "INVALID"),
        }
    }
}

/// Lexical state enumeration
///
/// Represents different lexical states the lexer can be in.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum LexicalState {
[#list lexerData.lexicalStates as state]
    /// Lexical state: ${state.name}
    ${state.name},
[/#list]
}

/// A single token in the input stream
///
/// Tokens are the atomic units produced by the lexer. Each token has:
/// - A type (e.g., INTEGER, PLUS, EOF)
/// - An image (the actual text matched)
/// - Position information (begin/end offsets)
/// - Optional links to previous/next tokens
#[derive(Debug, Clone)]
pub struct Token {
    /// The type of this token
    pub token_type: TokenType,
    /// The actual text of this token
    pub image: String,
    /// Absolute offset in the input where this token begins
    pub begin_offset: usize,
    /// Absolute offset in the input where this token ends
    pub end_offset: usize,
    /// Index of the next token (if any)
    pub next: Option<usize>,
    /// Index of the previous token (if any)
    pub previous: Option<usize>,
[#if settings.lexerUsesParser]
    /// Back-reference to parser (if needed)
    pub parser_ref: Option<usize>,
[/#if]
}

impl Token {
    /// Create a new token
    pub fn new(
        token_type: TokenType,
        image: String,
        begin_offset: usize,
        end_offset: usize,
    ) -> Self {
        Token {
            token_type,
            image,
            begin_offset,
            end_offset,
            next: None,
            previous: None,
[#if settings.lexerUsesParser]
            parser_ref: None,
[/#if]
        }
    }

    /// Get the length of this token in characters
    pub fn len(&self) -> usize {
        self.end_offset.saturating_sub(self.begin_offset)
    }

    /// Check if this token is empty
    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    /// Check if this token is of a specific type
    pub fn is_type(&self, token_type: TokenType) -> bool {
        self.token_type == token_type
    }

    /// Check if this token is one of the specified types
    pub fn is_one_of(&self, types: &[TokenType]) -> bool {
        types.contains(&self.token_type)
    }
}

impl fmt::Display for Token {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{}: \"{}\"", self.token_type, self.image)
    }
}

/// Token source trait for tracking token locations
pub trait TokenSource {
    /// Get the line number for a given offset
    fn get_line_from_offset(&self, offset: usize) -> usize;

    /// Get the column number for a given offset
    fn get_column_from_offset(&self, offset: usize) -> usize;
}
